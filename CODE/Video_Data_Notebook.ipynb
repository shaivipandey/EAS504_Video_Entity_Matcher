{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Data EM Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "###################################################################\n",
    "#KEY!!!! VARIABLE PREVENTS OVERWRITING LABELED SAMPLED DATA\n",
    "###################################################################\n",
    "GENERATE_NEW_LABELED_DATA = False\n",
    "GENERATE_NEW_TEST_TRAIN_DATA = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching two tables typically consists of the following three steps:\n",
    "\n",
    "** 1. Reading the input tables **\n",
    "\n",
    "** 2. Blocking the input tables to get a candidate set **\n",
    "\n",
    "** 3. Matching the tuple pairs in the candidate set **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read input tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../DATA/imdb3_neg_nan.csv\n"
     ]
    }
   ],
   "source": [
    "# Get the paths\n",
    "path_A = '../DATA/imdb3_neg_nan.csv'\n",
    "path_B = '../DATA/thenumbers3_neg_nan.csv'\n",
    "print(path_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"py_entitymatching.io.parsers\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>mpaa</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genres</th>\n",
       "      <th>director</th>\n",
       "      <th>stars</th>\n",
       "      <th>gross</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Toy Story 2</td>\n",
       "      <td>1999</td>\n",
       "      <td>G</td>\n",
       "      <td>92 min</td>\n",
       "      <td>Animation, Adventure, Comedy</td>\n",
       "      <td>John Lasseter</td>\n",
       "      <td>Ash Brannon,Lee Unkrich,Tom Hanks,Tim Allen,Joan Cusack,Kelsey Grammer</td>\n",
       "      <td>$245.85M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Outlaw Josey Wales</td>\n",
       "      <td>1976</td>\n",
       "      <td>PG</td>\n",
       "      <td>135 min</td>\n",
       "      <td>Western</td>\n",
       "      <td>Clint Eastwood</td>\n",
       "      <td>Clint Eastwood,Sondra Locke,Chief Dan George,Bill McKinney</td>\n",
       "      <td>$31.80M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Monsters, Inc.</td>\n",
       "      <td>2001</td>\n",
       "      <td>G</td>\n",
       "      <td>92 min</td>\n",
       "      <td>Animation, Adventure, Comedy</td>\n",
       "      <td>Pete Docter</td>\n",
       "      <td>David Silverman,Lee Unkrich,Billy Crystal,John Goodman,Mary Gibbs,Steve Buscemi</td>\n",
       "      <td>$289.92M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In the Heat of the Night</td>\n",
       "      <td>1967</td>\n",
       "      <td>Not Rated</td>\n",
       "      <td>110 min</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>Norman Jewison</td>\n",
       "      <td>Sidney Poitier,Rod Steiger,Warren Oates,Lee Grant</td>\n",
       "      <td>$24.38M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Chungking Express</td>\n",
       "      <td>1994</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>102 min</td>\n",
       "      <td>Crime, Drama, Romance</td>\n",
       "      <td>Kar-Wai Wong</td>\n",
       "      <td>Brigitte Lin,Takeshi Kaneshiro,Tony Chiu-Wai Leung,Faye Wong</td>\n",
       "      <td>$0.60M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                     title  year       mpaa  runtime  \\\n",
       "0   0               Toy Story 2  1999          G   92 min   \n",
       "1   1    The Outlaw Josey Wales  1976         PG  135 min   \n",
       "2   2            Monsters, Inc.  2001          G   92 min   \n",
       "3   3  In the Heat of the Night  1967  Not Rated  110 min   \n",
       "4   4         Chungking Express  1994      PG-13  102 min   \n",
       "\n",
       "                         genres        director  \\\n",
       "0  Animation, Adventure, Comedy   John Lasseter   \n",
       "1                       Western  Clint Eastwood   \n",
       "2  Animation, Adventure, Comedy     Pete Docter   \n",
       "3         Crime, Drama, Mystery  Norman Jewison   \n",
       "4         Crime, Drama, Romance    Kar-Wai Wong   \n",
       "\n",
       "                                                                             stars  \\\n",
       "0           Ash Brannon,Lee Unkrich,Tom Hanks,Tim Allen,Joan Cusack,Kelsey Grammer   \n",
       "1                       Clint Eastwood,Sondra Locke,Chief Dan George,Bill McKinney   \n",
       "2  David Silverman,Lee Unkrich,Billy Crystal,John Goodman,Mary Gibbs,Steve Buscemi   \n",
       "3                                Sidney Poitier,Rod Steiger,Warren Oates,Lee Grant   \n",
       "4                     Brigitte Lin,Takeshi Kaneshiro,Tony Chiu-Wai Leung,Faye Wong   \n",
       "\n",
       "      gross  \n",
       "0  $245.85M  \n",
       "1   $31.80M  \n",
       "2  $289.92M  \n",
       "3   $24.38M  \n",
       "4    $0.60M  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv files as dataframes and set the key attribute in the dataframe\n",
    "A = em.read_csv_metadata(path_A, key='id')\n",
    "B = em.read_csv_metadata(path_B, key='id')\n",
    "A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>mpaa</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genres</th>\n",
       "      <th>director</th>\n",
       "      <th>stars</th>\n",
       "      <th>gross</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Effects</td>\n",
       "      <td>2005</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Dusty Nelson</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ek Haseena Thi Ek Deewana Tha</td>\n",
       "      <td>2017</td>\n",
       "      <td>-1</td>\n",
       "      <td>105 minutes</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Suneel Darshan</td>\n",
       "      <td>-1</td>\n",
       "      <td>$149,491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ekk Albela</td>\n",
       "      <td>2016</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Shekhar Sartandel</td>\n",
       "      <td>-1</td>\n",
       "      <td>$1,907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Daisy Chain</td>\n",
       "      <td>2009</td>\n",
       "      <td>R</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dali &amp; I: The Surreal Story</td>\n",
       "      <td>2011</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Andrew Niccol</td>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                          title  year mpaa      runtime genres  \\\n",
       "0   0                        Effects  2005   -1           -1     -1   \n",
       "1   1  Ek Haseena Thi Ek Deewana Tha  2017   -1  105 minutes  Drama   \n",
       "2   2                     Ekk Albela  2016   -1           -1  Drama   \n",
       "3   3                The Daisy Chain  2009    R           -1     -1   \n",
       "4   4    Dali & I: The Surreal Story  2011   -1           -1  Drama   \n",
       "\n",
       "            director      stars     gross  \n",
       "0       Dusty Nelson         -1        -1  \n",
       "1     Suneel Darshan         -1  $149,491  \n",
       "2  Shekhar Sartandel         -1    $1,907  \n",
       "3                 -1         -1        -1  \n",
       "4      Andrew Niccol  Al Pacino        -1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "title       0\n",
       "year        0\n",
       "mpaa        0\n",
       "runtime     0\n",
       "genres      0\n",
       "director    0\n",
       "stars       0\n",
       "gross       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "title       0\n",
       "year        0\n",
       "mpaa        0\n",
       "runtime     0\n",
       "genres      0\n",
       "director    0\n",
       "stars       0\n",
       "gross       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tuples in A: 4291\n",
      "Number of tuples in B: 31006\n",
      "Number of tuples in A X B (i.e the cartesian product): 133046746\n"
     ]
    }
   ],
   "source": [
    "print('Number of tuples in A: ' + str(len(A)))\n",
    "print('Number of tuples in B: ' + str(len(B)))\n",
    "print('Number of tuples in A X B (i.e the cartesian product): ' + str(len(A)*len(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('id', 'id')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the keys of the input tables\n",
    "em.get_key(A), em.get_key(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the tables are large we can downsample the tables like this\n",
    "A1, B1 = em.down_sample(A, B, 200, 1, show_progress=False)\n",
    "len(A1), len(B1)\n",
    "\n",
    "# But for the purposes of this notebook, we will use the entire table A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block tables to get candidate set\n",
    "\n",
    "Before we do the matching, we would like to remove the obviously non-matching tuple pairs from the input tables to reduce computational complexity. This would reduce the number of tuple pairs considered for matching.\n",
    "*py_entitymatching* provides four different blockers: (1) attribute equivalence, (2) overlap, (3) rule-based, and (4) black-box. We can use a mix and match of these blockers to form a blocking sequence applied to input tables.\n",
    "\n",
    "For the matching problem at hand, we know that two movies with different titles will not match. So we decide the apply blocking over names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Blocking plan\n",
    "\n",
    "# A, B -- overlap blocker [title] --------------------|---> candidate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:38\n"
     ]
    }
   ],
   "source": [
    "# Create an overlap blocker\n",
    "ab = em.OverlapBlocker()\n",
    "\n",
    "# Block using 'title' attribute\n",
    "C1 = ab.block_tables(A, B, 'title', 'title', \n",
    "                    l_output_attrs=['title', 'year', 'mpaa', 'runtime', 'genres', 'director', 'stars', 'gross'], \n",
    "                    r_output_attrs=['title', 'year', 'mpaa', 'runtime', 'genres', 'director', 'stars', 'gross'],\n",
    "                    overlap_size=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10956134"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug blocker output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tuple pairs considered for matching is reduced to 10,956,134 (from 133,046,746, approx. 8%), but we would want to make sure that the blocker did not drop any potential matches. We could debug the blocker output in *py_entitymatching* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 44.95 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Debug blocker output\n",
    "startTime = time.time()\n",
    "dbg = em.debug_blocker(C1, A, B, output_size=200, attr_corres=[('title','title'), ('year', 'year')])\n",
    "endTime = time.time()\n",
    "print(\"Total time: %.2f seconds.\"%(endTime-startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>ltable_id</th>\n",
       "      <th>rtable_id</th>\n",
       "      <th>ltable_title</th>\n",
       "      <th>ltable_year</th>\n",
       "      <th>rtable_title</th>\n",
       "      <th>rtable_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2965</td>\n",
       "      <td>17445</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Ginger &amp; Rosa</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2965</td>\n",
       "      <td>17842</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Harry &amp; Son</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2965</td>\n",
       "      <td>17844</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Harry &amp; Snowman</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2965</td>\n",
       "      <td>19318</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Lambert &amp; Stamp</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2965</td>\n",
       "      <td>19600</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>David &amp; Layla</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2965</td>\n",
       "      <td>19796</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Hansel &amp; Gretel</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2965</td>\n",
       "      <td>20161</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Q &amp; A</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2965</td>\n",
       "      <td>23293</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Starsky &amp; Hutch</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2965</td>\n",
       "      <td>23439</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Spring &amp; Arnaud</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2965</td>\n",
       "      <td>25285</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Town &amp; Country</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2965</td>\n",
       "      <td>26599</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Withnail &amp; I</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2965</td>\n",
       "      <td>26620</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Rodeo &amp; Juliet</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2965</td>\n",
       "      <td>26824</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Rock &amp; Rule</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2965</td>\n",
       "      <td>28999</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Water &amp; Power</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2965</td>\n",
       "      <td>29445</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Wallace &amp; Gromit</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2965</td>\n",
       "      <td>29519</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Up &amp; Down</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2965</td>\n",
       "      <td>30408</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Yossi &amp; Jagger</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>3038</td>\n",
       "      <td>236</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Burke &amp; Hare</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2661</td>\n",
       "      <td>29762</td>\n",
       "      <td>Chef</td>\n",
       "      <td>2014</td>\n",
       "      <td>Ragnarok</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2661</td>\n",
       "      <td>30538</td>\n",
       "      <td>Chef</td>\n",
       "      <td>2014</td>\n",
       "      <td>Ungli</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2661</td>\n",
       "      <td>30870</td>\n",
       "      <td>Chef</td>\n",
       "      <td>2014</td>\n",
       "      <td>Unbroken</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2965</td>\n",
       "      <td>236</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Burke &amp; Hare</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2965</td>\n",
       "      <td>782</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Bride &amp; Prejudice</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2965</td>\n",
       "      <td>4163</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Love &amp; Taxes</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2965</td>\n",
       "      <td>4262</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2965</td>\n",
       "      <td>4263</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Love &amp; Friendship</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2965</td>\n",
       "      <td>6232</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Plunkett &amp; Macleane</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>2965</td>\n",
       "      <td>6337</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Cloak &amp; Dagger</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2965</td>\n",
       "      <td>6586</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Gun &amp; Goal</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2965</td>\n",
       "      <td>13033</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Henry &amp; Me</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>2965</td>\n",
       "      <td>13036</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Henry &amp; June</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>2965</td>\n",
       "      <td>13040</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Hemingway &amp; Gellhorn</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>2965</td>\n",
       "      <td>14788</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Heaven &amp; Earth</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>2965</td>\n",
       "      <td>15471</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>AimÃ©e &amp; Jaguar</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>2965</td>\n",
       "      <td>16789</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>In &amp; Out</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>2965</td>\n",
       "      <td>16873</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Cas &amp; Dylan</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>2965</td>\n",
       "      <td>16878</td>\n",
       "      <td>Advise &amp; Consent</td>\n",
       "      <td>1962</td>\n",
       "      <td>Carter &amp; June</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>3038</td>\n",
       "      <td>23293</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Starsky &amp; Hutch</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>3038</td>\n",
       "      <td>23439</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Spring &amp; Arnaud</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>3038</td>\n",
       "      <td>25285</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Town &amp; Country</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>3038</td>\n",
       "      <td>26599</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Withnail &amp; I</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>3038</td>\n",
       "      <td>26620</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Rodeo &amp; Juliet</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>3038</td>\n",
       "      <td>26824</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Rock &amp; Rule</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>3038</td>\n",
       "      <td>28999</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Water &amp; Power</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>3038</td>\n",
       "      <td>29445</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Wallace &amp; Gromit</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>3038</td>\n",
       "      <td>29519</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Up &amp; Down</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>3038</td>\n",
       "      <td>30408</td>\n",
       "      <td>Love &amp; Mercy</td>\n",
       "      <td>2014</td>\n",
       "      <td>Yossi &amp; Jagger</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>3431</td>\n",
       "      <td>236</td>\n",
       "      <td>Oliver &amp; Company</td>\n",
       "      <td>1988</td>\n",
       "      <td>Burke &amp; Hare</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>3431</td>\n",
       "      <td>782</td>\n",
       "      <td>Oliver &amp; Company</td>\n",
       "      <td>1988</td>\n",
       "      <td>Bride &amp; Prejudice</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>3431</td>\n",
       "      <td>4163</td>\n",
       "      <td>Oliver &amp; Company</td>\n",
       "      <td>1988</td>\n",
       "      <td>Love &amp; Taxes</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _id  ltable_id  rtable_id      ltable_title ltable_year  \\\n",
       "0     0       2965      17445  Advise & Consent        1962   \n",
       "1     1       2965      17842  Advise & Consent        1962   \n",
       "2     2       2965      17844  Advise & Consent        1962   \n",
       "3     3       2965      19318  Advise & Consent        1962   \n",
       "4     4       2965      19600  Advise & Consent        1962   \n",
       "5     5       2965      19796  Advise & Consent        1962   \n",
       "6     6       2965      20161  Advise & Consent        1962   \n",
       "7     7       2965      23293  Advise & Consent        1962   \n",
       "8     8       2965      23439  Advise & Consent        1962   \n",
       "9     9       2965      25285  Advise & Consent        1962   \n",
       "10   10       2965      26599  Advise & Consent        1962   \n",
       "11   11       2965      26620  Advise & Consent        1962   \n",
       "12   12       2965      26824  Advise & Consent        1962   \n",
       "13   13       2965      28999  Advise & Consent        1962   \n",
       "14   14       2965      29445  Advise & Consent        1962   \n",
       "15   15       2965      29519  Advise & Consent        1962   \n",
       "16   16       2965      30408  Advise & Consent        1962   \n",
       "17   17       3038        236      Love & Mercy        2014   \n",
       "18   18       2661      29762              Chef        2014   \n",
       "19   19       2661      30538              Chef        2014   \n",
       "20   20       2661      30870              Chef        2014   \n",
       "21   21       2965        236  Advise & Consent        1962   \n",
       "22   22       2965        782  Advise & Consent        1962   \n",
       "23   23       2965       4163  Advise & Consent        1962   \n",
       "24   24       2965       4262  Advise & Consent        1962   \n",
       "25   25       2965       4263  Advise & Consent        1962   \n",
       "26   26       2965       6232  Advise & Consent        1962   \n",
       "27   27       2965       6337  Advise & Consent        1962   \n",
       "28   28       2965       6586  Advise & Consent        1962   \n",
       "29   29       2965      13033  Advise & Consent        1962   \n",
       "30   30       2965      13036  Advise & Consent        1962   \n",
       "31   31       2965      13040  Advise & Consent        1962   \n",
       "32   32       2965      14788  Advise & Consent        1962   \n",
       "33   33       2965      15471  Advise & Consent        1962   \n",
       "34   34       2965      16789  Advise & Consent        1962   \n",
       "35   35       2965      16873  Advise & Consent        1962   \n",
       "36   36       2965      16878  Advise & Consent        1962   \n",
       "37   37       3038      23293      Love & Mercy        2014   \n",
       "38   38       3038      23439      Love & Mercy        2014   \n",
       "39   39       3038      25285      Love & Mercy        2014   \n",
       "40   40       3038      26599      Love & Mercy        2014   \n",
       "41   41       3038      26620      Love & Mercy        2014   \n",
       "42   42       3038      26824      Love & Mercy        2014   \n",
       "43   43       3038      28999      Love & Mercy        2014   \n",
       "44   44       3038      29445      Love & Mercy        2014   \n",
       "45   45       3038      29519      Love & Mercy        2014   \n",
       "46   46       3038      30408      Love & Mercy        2014   \n",
       "47   47       3431        236  Oliver & Company        1988   \n",
       "48   48       3431        782  Oliver & Company        1988   \n",
       "49   49       3431       4163  Oliver & Company        1988   \n",
       "\n",
       "            rtable_title  rtable_year  \n",
       "0          Ginger & Rosa         2013  \n",
       "1            Harry & Son         1984  \n",
       "2        Harry & Snowman         2016  \n",
       "3        Lambert & Stamp         2015  \n",
       "4          David & Layla         2007  \n",
       "5        Hansel & Gretel         2002  \n",
       "6                  Q & A         1990  \n",
       "7        Starsky & Hutch         2004  \n",
       "8        Spring & Arnaud         2016  \n",
       "9         Town & Country         2001  \n",
       "10          Withnail & I         1987  \n",
       "11        Rodeo & Juliet         2016  \n",
       "12           Rock & Rule         1983  \n",
       "13         Water & Power         2014  \n",
       "14      Wallace & Gromit         1996  \n",
       "15             Up & Down         2005  \n",
       "16        Yossi & Jagger         2003  \n",
       "17          Burke & Hare         2011  \n",
       "18              Ragnarok         2014  \n",
       "19                 Ungli         2014  \n",
       "20              Unbroken         2014  \n",
       "21          Burke & Hare         2011  \n",
       "22     Bride & Prejudice         2005  \n",
       "23          Love & Taxes         2017  \n",
       "24          Love & Mercy         2015  \n",
       "25     Love & Friendship         2016  \n",
       "26   Plunkett & Macleane         1999  \n",
       "27        Cloak & Dagger         1984  \n",
       "28            Gun & Goal         2015  \n",
       "29            Henry & Me         2014  \n",
       "30          Henry & June         1990  \n",
       "31  Hemingway & Gellhorn         2013  \n",
       "32        Heaven & Earth         1993  \n",
       "33       AimÃ©e & Jaguar         2000  \n",
       "34              In & Out         1997  \n",
       "35           Cas & Dylan         2015  \n",
       "36         Carter & June         2017  \n",
       "37       Starsky & Hutch         2004  \n",
       "38       Spring & Arnaud         2016  \n",
       "39        Town & Country         2001  \n",
       "40          Withnail & I         1987  \n",
       "41        Rodeo & Juliet         2016  \n",
       "42           Rock & Rule         1983  \n",
       "43         Water & Power         2014  \n",
       "44      Wallace & Gromit         1996  \n",
       "45             Up & Down         2005  \n",
       "46        Yossi & Jagger         2003  \n",
       "47          Burke & Hare         2011  \n",
       "48     Bride & Prejudice         2005  \n",
       "49          Love & Taxes         2017  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few tuple pairs from the debug_blocker's output\n",
    "dbg.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the debug blocker's output we observe that the current blocker retains quite a lot of obvious mismatches. We would want to update the blocking sequence to drop few more of these obvious mismatches.\n",
    "\n",
    "For the considered dataset, we know that for the movies to match the year of release must overlap between them. We could use overlap blocker for this purpose. Finally, we would want to find the intersection of the outputs from the two overlap blockers to get a consolidated candidate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated blocking sequence\n",
    "# A, B ------ overlap blocker [title] -----> C1--\n",
    "#                                                     \n",
    "# C1 ------ overlap blocker [year] --------> C2--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##################            ] 100% | ETA: 00:01:22"
     ]
    }
   ],
   "source": [
    "# Create overlap blocker\n",
    "ob = em.OverlapBlocker()\n",
    "\n",
    "# Block tables using 'year' attribute \n",
    "C2 = ob.block_candset(C1, 'year', 'year' \n",
    "                    )\n",
    "len(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first two rows from C2\n",
    "C2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We similarily reduce number of potential tuple pairs by matching mpaa ratings of the movies as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated blocking sequence\n",
    "# A, B ------ overlap blocker [title] -----> C1--\n",
    "#                                                     \n",
    "# C1 ------ overlap blocker [year] --------> C2--\n",
    "#\n",
    "# C2 ------ overlap blocker [mpaa] --------> C3--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overlap blocker\n",
    "ob = em.OverlapBlocker()\n",
    "\n",
    "# Block tables using 'name' attribute \n",
    "C3 = ob.block_candset(C2, 'mpaa', 'mpaa' \n",
    "                    )\n",
    "len(C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first two rows from C3\n",
    "C3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that a lot of potential matches in our first overlap blocker are due to the word *the* being matched between two movie titles. We wish to filter out such tuple pairs. Hence we write a customized black box blocker additionally into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated blocking sequence\n",
    "# A, B ------ overlap blocker [title] -----> C1--\n",
    "#                                                     \n",
    "# C1 ------ overlap blocker [year] --------> C2--\n",
    "#\n",
    "# C2 ------ overlap blocker [mpaa] --------> C3--\n",
    "#\n",
    "# C3 ------ black box [title match more than just 'the'] --------> C4--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create blocker to block tuples that only match based on 'the'.\n",
    "def overlap_ignoring_words(ltuple, rtuple):\n",
    "    words = set(['the'])\n",
    "    # Remove ignore words set from strings.\n",
    "    lostring = ltuple['title'].lower().split()\n",
    "    rostring = rtuple['title'].lower().split()\n",
    "    l_tokens = []\n",
    "    r_tokens = []\n",
    "    for word in lostring:\n",
    "        if word.strip() not in words:\n",
    "            l_tokens.append(word.strip())\n",
    "    for word in rostring:\n",
    "        if word.strip() not in words:\n",
    "            r_tokens.append(word.strip())\n",
    "    l_tokens = set(l_tokens)\n",
    "    r_tokens = set(r_tokens)\n",
    "        \n",
    "    # Compute overlap.\n",
    "    #l_tokens = set(list(map(lambda item: item.strip(), lstring.split())))\n",
    "    #r_tokens = set(list(map(lambda item: item.strip(), rstring.split())))\n",
    "    intersection = l_tokens.intersection(r_tokens)\n",
    "    if len(intersection) >= 1:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "# Create and apply blocker.\n",
    "bb = em.BlackBoxBlocker()\n",
    "bb.set_black_box_function(overlap_ignoring_words)\n",
    "C4 = bb.block_candset(C3)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(C4)\n",
    "#Save all tuple pairs remainng after blocking step\n",
    "em.to_csv_metadata(C4, '../DATA/AllTuplePairs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can proceed with the matching step now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching tuple pairs in the candidate set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we would want to match the tuple pairs in the candidate set. Specifically, we use learning-based method for matching purposes.\n",
    "This typically involves the following five steps:\n",
    "1. Sampling and labeling the candidate set\n",
    "2. Splitting the labeled data into development and evaluation set\n",
    "3. Selecting the best learning based matcher using the development set\n",
    "4. Evaluating the selected matcher using the evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and labeling the candidate set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we randomly sample 600 tuple pairs for labeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample  candidate set\n",
    "S = em.sample_table(C4, 600)\n",
    "S.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_NEW_LABELED_DATA:\n",
    "    # Label S interactively. \n",
    "    G = em.label_table(S, 'gold')\n",
    "    # SAVE! TIS A BITCH TO LABEL S AGAIN, AND IT NEEDS TO BE LABELED WITHIN THE UI!\n",
    "    em.to_csv_metadata(G, '../DATA/G.csv')\n",
    "else:\n",
    "    # Load the dataset.\n",
    "    G = em.read_csv_metadata('../DATA/G.csv', \n",
    "                         key='_id',\n",
    "                         ltable=A, rtable=B, \n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the labeled data into development and evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we split the labeled data into two sets: development (I) and evaluation (J). Specifically, the development set is used to come up with the best learning-based matcher and the evaluation set used to evaluate the selected matcher on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_NEW_TEST_TRAIN_DATA:\n",
    "    # Split S into development set (I) and evaluation set (J)\n",
    "    IJ = em.split_train_test(G, train_proportion=0.7, random_state=5)\n",
    "    I = IJ['train']\n",
    "    J = IJ['test']\n",
    "    print(len(I[I['gold']==1]))\n",
    "    print(len(J[J['gold']==1]))\n",
    "    em.to_csv_metadata(I, '../DATA/I.csv')\n",
    "    em.to_csv_metadata(J, '../DATA/J.csv')\n",
    "else:\n",
    "    # Load the dataset.\n",
    "    I = em.read_csv_metadata('../DATA/I.csv', \n",
    "                         key='_id',\n",
    "                         ltable=A, rtable=B, \n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id')\n",
    "    J = em.read_csv_metadata('../DATA/J.csv', \n",
    "                         key='_id',\n",
    "                         ltable=A, rtable=B, \n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the best learning-based matcher "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the best learning-based matcher typically involves the following steps:\n",
    "\n",
    "1. Creating a set of learning-based matchers\n",
    "2. Creating features\n",
    "3. Converting the development set into feature vectors\n",
    "4. Selecting the best learning-based matcher using k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a set of learning-based matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of ML-matchers\n",
    "dt = em.DTMatcher(name='DecisionTree', random_state=0)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher(name='NaiveBayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a set of features for the development set. *py_entitymatching* provides a way to automatically generate features based on the attributes in the input tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features\n",
    "feature_table = em.get_features_for_matching(A, B, validate_inferred_attr_types=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the names of the features generated\n",
    "feature_table['feature_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the development set to feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the I into a set of feature vectors using F\n",
    "H = em.extract_feature_vecs(I, \n",
    "                            feature_table=feature_table, \n",
    "                            attrs_after='gold',\n",
    "                            show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "H.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the best matcher using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we select the best matcher using k-fold cross-validation. We use five fold cross validation and use 'F1 score' metric to select the best matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best ML matcher using CV\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg, nb], table=H, \n",
    "        exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'],\n",
    "        k=5,\n",
    "        target_attr='gold', metric_to_select_matcher='f1', random_state=0)\n",
    "result['cv_stats']#Training set performance measure using internal CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the random forrest model outperforms others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluating the matching output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the matching outputs for the evaluation set typically involves the following four steps:\n",
    "1. Converting the evaluation set to feature vectors\n",
    "2. Training matcher using the feature vectors extracted from the development set\n",
    "3. Predicting the evaluation set using the trained matcher\n",
    "4. Evaluating the predicted matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the evaluation set to  feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we convert to the feature vectors (using the feature table and the evaluation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert J into a set of feature vectors using feature table\n",
    "L = em.extract_feature_vecs(J, feature_table=feature_table,\n",
    "                            attrs_after='gold', show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the selected matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the matcher using all of the feature vectors from the development set. The selected model is Random Forrest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using feature vectors from I \n",
    "rf.fit(table=H, #the entire development set\n",
    "       exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "       target_attr='gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we predict the matches for the evaluation set (using the feature vectors extracted from it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on L \n",
    "predictions = rf.predict(table=L,#The entire evaluation set\n",
    "                         exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "              append=True, target_attr='predicted', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the accuracy of predicted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the predictions\n",
    "eval_result = em.eval_matches(predictions, 'gold', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using feature vectors from I \n",
    "dt.fit(table=H, #the entire development set\n",
    "       exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "       target_attr='gold')\n",
    "# Train using feature vectors from I \n",
    "rf.fit(table=H, #the entire development set\n",
    "       exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "       target_attr='gold')\n",
    "# Train using feature vectors from I \n",
    "svm.fit(table=H, #the entire development set\n",
    "       exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "       target_attr='gold')\n",
    "# Train using feature vectors from I \n",
    "lg.fit(table=H, #the entire development set\n",
    "       exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "       target_attr='gold')\n",
    "# Train using feature vectors from I \n",
    "ln.fit(table=H, #the entire development set\n",
    "       exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "       target_attr='gold')\n",
    "# Train using feature vectors from I \n",
    "nb.fit(table=H, #the entire development set\n",
    "       exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "       target_attr='gold')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform predictions for every model and output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on L\n",
    "models = [dt, rf, svm, ln, lg, nb]\n",
    "modelNames = ['dt', 'rf', 'svm', 'ln', 'lg', 'nb']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    predictions = model.predict(table=L,#The entire evaluation set\n",
    "                         exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'gold'], \n",
    "              append=True, target_attr='predicted', inplace=False)\n",
    "    eval_result = em.eval_matches(predictions, 'gold', 'predicted')\n",
    "    print(modelNames[i])\n",
    "    em.print_eval_summary(eval_result)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
